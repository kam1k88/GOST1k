import sys; sys.stdout.reconfigure(encoding='utf-8')
import os
import re
import asyncio
import httpx
import numpy as np
import time
from datetime import datetime
from dotenv import load_dotenv
from FlagEmbedding import BGEM3FlagModel
from sentence_transformers import CrossEncoder
import chromadb
import torch

# === –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===
os.environ["CHROMA_TELEMETRY"] = "False"
load_dotenv()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_DIR = os.path.join(BASE_DIR, "chroma_db")
COLLECTION_NAME = "gost1k"
LOG_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, "gost1k.log")

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
def log_event(msg):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{ts}] {msg}\n")

# === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ ===
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"[‚öôÔ∏è] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE.upper()}")

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π ===
embedder = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True, device=DEVICE)
reranker = CrossEncoder("BAAI/bge-reranker-base", device=DEVICE)

# === –ü—Ä–æ–≥—Ä–µ–≤ GPU ===
_ = embedder.encode(["warmup"], batch_size=1, max_length=128)
torch.cuda.empty_cache()

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Chroma ===
client = chromadb.PersistentClient(path=CHROMA_DIR)
collection = client.get_or_create_collection(COLLECTION_NAME, embedding_function=None)

# === –£—Ç–∏–ª–∏—Ç—ã ===
def clean_text(t):
    t = re.sub(r"<[^>]+>", " ", t)
    t = re.sub(r"\s+", " ", t)
    return t.strip()

def normalize_query(q: str) -> str:
    return f"query: {q.strip().lower()}"

def format_doc(d: str) -> str:
    return f"passage: {d.strip()}"

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ===
def check_collection():
    try:
        print("[üß©] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Chroma...")
        count = collection.count()
        print(f"[üì¶] –í—Å–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {count}")
        peek = collection.peek()
        if not peek or "documents" not in peek or not peek["documents"]:
            print("[‚ö†Ô∏è] –ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞.")
            return
        doc = peek["documents"][0][0] if isinstance(peek["documents"][0], list) else peek["documents"][0]
        meta = peek["metadatas"][0][0] if isinstance(peek["metadatas"][0], list) else peek["metadatas"][0]
        print("[üìÑ] –ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n", clean_text(doc[:300]))
        print("[üóÇÔ∏è] –ü—Ä–∏–º–µ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:\n", meta)
    except Exception as e:
        print(f"[‚ùå] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")

# === –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (dense + sparse fusion) ===
def hybrid_search(q, top_k=50, alpha=0.65):
    """
    alpha - –≤–µ—Å dense. 0.65 –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –ì–û–°–¢–æ–≤ (—Å–µ–º–∞–Ω—Ç–∏–∫–∞ –≤–∞–∂–Ω–µ–µ, –Ω–æ —Ç–µ—Ä–º–∏–Ω—ã —É—á–∏—Ç—ã–≤–∞–µ–º).
    """
    try:
        t0 = time.time()

        # 1) –≥–∏–±—Ä–∏–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–ø—Ä–æ—Å–∞
        #   –≤ FlagEmbedding>=1.3 –Ω–µ –Ω—É–∂–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å normalize_embeddings,
        #   –∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å return_dense, return_sparse
        res = embedder.encode([q], return_dense=True, return_sparse=True)
        dense_vec = res["dense_vecs"]
        sparse_weights = res["lexical_weights"][0] if "lexical_weights" in res else {}

        # 2) dense-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –∏–∑ Chroma
        cres = collection.query(query_embeddings=dense_vec, n_results=top_k * 2)
        docs0 = cres.get("documents", [[]])[0]
        metas0 = cres.get("metadatas", [[]])[0]
        dists0 = cres.get("distances", [[]])[0]

        if not docs0:
            print("[‚ö†Ô∏è] –ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.")
            return []

        # === dense part ===
        dense_scores = {}
        for m, dist in zip(metas0, dists0):
            sim = 1.0 - float(dist)
            dense_scores[m["source"]] = max(dense_scores.get(m["source"], 0.0), sim)

        # === sparse part ===
        sparse_scores = {}
        for doc, meta in zip(docs0, metas0):
            if not sparse_weights:
                continue
            score = sum(sparse_weights.get(tok, 0.0) for tok in doc.split())
            sparse_scores[meta["source"]] = max(sparse_scores.get(meta["source"], 0.0), score)

        # === fusion ===
        fused = {}
        keys = set(dense_scores.keys()) | set(sparse_scores.keys())
        for k in keys:
            fused[k] = alpha * dense_scores.get(k, 0.0) + (1 - alpha) * sparse_scores.get(k, 0.0)

        # === –æ—Ç–ª–∞–¥–æ—á–Ω—ã–µ —Ç–æ–ø—ã ===
        if fused:
            top_dense = sorted(dense_scores.items(), key=lambda x: x[1], reverse=True)[:3]
            top_sparse = sorted(sparse_scores.items(), key=lambda x: x[1], reverse=True)[:3]
            print("\nüß† Top dense:")
            [print(f"  {n}: {s:.3f}") for n, s in top_dense]
            print("ü™∂ Top sparse:")
            [print(f"  {n}: {s:.3f}") for n, s in top_sparse]

        # === —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥ ===
        ranked_ids = [k for k, _ in sorted(fused.items(), key=lambda x: x[1], reverse=True)[:top_k]]
        docs = [{"text": d, "source": s} for d, s in zip(docs0, metas0) if s["source"] in ranked_ids]

        total_time = time.time() - t0
        log_event(f"[‚è±Ô∏è] BGE-M3 hybrid dense+sparse: {total_time:.2f} —Å–µ–∫ ({len(docs)} docs)")
        return docs

    except Exception as e:
        print(f"[‚ùå] –û—à–∏–±–∫–∞ hybrid_search: {e}")
        log_event(f"[‚ùå] –û—à–∏–±–∫–∞ hybrid_search: {e}")
        return []

# === –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ ===
def rerank_docs(q, docs):
    if not docs:
        return []
    t0 = time.time()
    pairs = [[normalize_query(q), format_doc(d["text"])] for d in docs]
    scores = reranker.predict(pairs)
    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    rerank_time = time.time() - t0
    log_event(f"[‚è±Ô∏è] Rerank: {rerank_time:.2f} —Å–µ–∫ ({len(docs)} docs)")
    return [r[0] for r in ranked]

# === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Ollama ===
async def ollama_generate(prompt, model=None):
    torch.cuda.empty_cache()
    model = model or os.getenv("OLLAMA_MODEL", "qwen2.5:7b-instruct-q4_K_M")
    async with httpx.AsyncClient(timeout=300.0) as c:
        try:
            ollama_host = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
            t0 = time.time()
            r = await c.post(f"{ollama_host}/api/generate",
                             json={"model": model, "prompt": prompt, "stream": False})
            data = r.json()
            total_time = time.time() - t0
            log_event(f"[‚è±Ô∏è] Ollama –æ—Ç–≤–µ—Ç: {total_time:.2f} —Å–µ–∫")
            if "response" in data and data["response"].strip():
                return data["response"].strip()
            return data.get("output", "") or str(data)
        except Exception as e:
            return f"[‚ö†Ô∏è] Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}"
        finally:
            torch.cuda.empty_cache()

# === –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ RAG ===
async def answer(query: str):
    t0 = time.time()
    print(f"[üîç] –ó–∞–ø—Ä–æ—Å: {query}")
    sys_query = normalize_query(query)
    log_event(f"\n[USER QUERY] {query}")
    log_event(f"[SYSTEM QUERY] {sys_query}")

    docs = hybrid_search(sys_query)
    print(f"[~] –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(docs)}")

    if not docs:
        msg = "[‚ö†Ô∏è] –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."
        print(msg)
        log_event(msg)
        return msg

    reranked = rerank_docs(sys_query, docs)
    top_docs = reranked[:5]

    for i, d in enumerate(top_docs, 1):
        snippet = clean_text(d["text"][:150])
        src = d.get("source", "?")
        print(f"{i:>2}. {snippet} ... [source={src}]")

    context = "\n\n".join([clean_text(d["text"][:800]) for d in top_docs])

    intro = (
        "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –¢–ö 362 '–ó–∞—â–∏—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏'. "
        "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∏ —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–ì–û–°–¢, –°–¢–û, –†–î, –ø—Ä–∏–∫–∞–∑—ã, –ø–æ–ª–æ–∂–µ–Ω–∏—è). "
        "–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–µ –Ω–∏–∂–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–µ –æ—Å–Ω–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞. "
        "–ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–≤–µ—Ç–∞ ‚Äî —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º –ø—Ä—è–º–æ. "
        "–§–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É, —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Ñ–∞–∫—Ç–æ–≤ –∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏–∑ –ì–û–°–¢–æ–≤."
    )

    mode = "deep_analytic" if len(query.split()) > 15 else "structured"
    prompt = f"{intro}\n\n–í–æ–ø—Ä–æ—Å: {query}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–û—Ç–≤–µ—Ç:"

    ans = await ollama_generate(prompt)
    total_time = time.time() - t0
    print(f"\n[üí°] –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω ({total_time:.2f} —Å–µ–∫):\n{ans}")
    log_event(f"[ANSWER | {mode}] ({total_time:.2f} —Å–µ–∫)\n{ans}\n{'='*80}\n")
    return ans

# === CLI ===
if __name__ == "__main__":
    check_collection()
    q = "–∞—É–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Å–æ–±—ã—Ç–∏–π"
    asyncio.run(answer(q))
