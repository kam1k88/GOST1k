import os
from dotenv import load_dotenv
load_dotenv()
import re
import asyncio
import httpx
from datetime import datetime
from sentence_transformers import SentenceTransformer, CrossEncoder
import chromadb
import torch

# === –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===
os.environ["CHROMA_TELEMETRY"] = "False"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_DIR = os.path.join(BASE_DIR, "chroma_db")
COLLECTION_NAME = "gost1k"
LOG_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, "gost1k.log")

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
def log_event(msg):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{ts}] {msg}\n")

# === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ ===
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"[‚öôÔ∏è] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE.upper()}")

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π ===
embedder = SentenceTransformer("intfloat/multilingual-e5-small", device=DEVICE)
reranker = CrossEncoder("BAAI/bge-reranker-base", device=DEVICE)

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Chroma ===
client = chromadb.PersistentClient(path=CHROMA_DIR)
collection = client.get_or_create_collection(COLLECTION_NAME, embedding_function=None)

# === –£—Ç–∏–ª–∏—Ç—ã ===
def clean_text(t):
    t = re.sub(r"<[^>]+>", " ", t)
    t = re.sub(r"\s+", " ", t)
    return t.strip()

def normalize_query(q: str) -> str:
    """–î–æ–±–∞–≤–ª—è–µ—Ç E5-–ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    return f"query: {q.strip().lower()}"

def format_doc(d: str) -> str:
    return f"passage: {d.strip()}"


# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ===
def check_collection():
    try:
        print("[üß©] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Chroma...")
        count = collection.count()
        print(f"[üì¶] –í—Å–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {count}")
        peek = collection.peek()
        if not peek or "documents" not in peek or not peek["documents"]:
            print("[‚ö†Ô∏è] –ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
            return
        doc = peek["documents"][0][0] if isinstance(peek["documents"][0], list) else peek["documents"][0]
        meta = peek["metadatas"][0][0] if isinstance(peek["metadatas"][0], list) else peek["metadatas"][0]
        print("[üìÑ] –ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n", clean_text(doc[:300]))
        print("[üóÇÔ∏è] –ü—Ä–∏–º–µ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:\n", meta)
    except Exception as e:
        print(f"[‚ùå] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")


# === –ü–æ–∏—Å–∫ ===
def dense_query(q, top_k=50):
    """Dense-–ø–æ–∏—Å–∫ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º E5"""
    try:
        q_emb = embedder.encode([normalize_query(q)], normalize_embeddings=True)
        results = collection.query(query_embeddings=q_emb, n_results=top_k)
        if not results.get("documents") or not results["documents"][0]:
            print("[‚ö†Ô∏è] –ü—É—Å—Ç–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –∏–ª–∏ –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.")
            return []
        docs = [
            {"text": d, "source": s}
            for d, s in zip(results["documents"][0], results["metadatas"][0])
        ]
        return docs
    except Exception as e:
        print(f"[‚ùå] –û—à–∏–±–∫–∞ dense_query: {e}")
        return []


# === –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ ===
def rerank_docs(q, docs):
    if not docs:
        return []
    pairs = [[normalize_query(q), format_doc(d["text"])] for d in docs]
    scores = reranker.predict(pairs)
    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    return [r[0] for r in ranked]


# === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Ollama ===
async def ollama_generate(prompt, model="qwen2.5:7b-instruct-q4_K_M"):
    torch.cuda.empty_cache()
    async with httpx.AsyncClient(timeout=300.0) as c:
        try:
            ollama_host = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
            r = await c.post(
                f"{ollama_host}/api/generate",
		json={"model": model, "prompt": prompt, "stream": False},
            )
            data = r.json()
            if "response" in data and data["response"].strip():
                return data["response"].strip()
            if "message" in data and isinstance(data["message"], dict):
                return data["message"].get("content", "").strip()
            text = data.get("output", "") or str(data)
            if "CUDA error" in text:
                print("[‚ö†Ô∏è] CUDA error ‚Äî fallback ‚Üí Qwen2:1.5b-instruct")
                torch.cuda.empty_cache()
                return await ollama_generate(prompt, "Qwen2:1.5b-instruct")
            return text.strip() if text else "[‚ö†Ô∏è] –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç Ollama"
        except Exception as e:
            if "CUDA" in str(e):
                print("[‚ö†Ô∏è] –û—à–∏–±–∫–∞ CUDA ‚Äî fallback ‚Üí Qwen2:1.5b-instruct")
                torch.cuda.empty_cache()
                return await ollama_generate(prompt, "Qwen2:1.5b-instruct")
            return f"[‚ö†Ô∏è] Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}"
        finally:
            torch.cuda.empty_cache()


# === –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ RAG ===
async def answer(query: str):
    print(f"[üîç] –ó–∞–ø—Ä–æ—Å: {query}")
    sys_query = normalize_query(query)
    print(f"[üß†] –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ: {sys_query}")
    log_event(f"\n[USER QUERY] {query}")
    log_event(f"[SYSTEM QUERY] {sys_query}")

    docs = dense_query(sys_query)
    print(f"[~] –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(docs)}")
    log_event(f"[CHROMA] –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(docs)}")

    if not docs:
        msg = "[‚ö†Ô∏è] –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."
        print(msg)
        log_event(msg)
        return msg

    reranked = rerank_docs(sys_query, docs)
    top_docs = reranked[:5]

    print("\n[üèÜ] –¢–æ–ø-5 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤:")
    for i, d in enumerate(top_docs, 1):
        snippet = clean_text(d["text"][:150])
        src = d.get("source", "?")
        print(f"{i:>2}. {snippet} ...\n   [source={src}]")
        log_event(f"[DOC {i}] {snippet} | source={src}")

    context = "\n\n".join([clean_text(d["text"][:800]) for d in top_docs])

    intro = (
        "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. "
        "–û—Ç–≤–µ—á–∞–π **—Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ**, —Å—Ç—Ä–æ–≥–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¢–ö 362 '–ó–∞—â–∏—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏'. "
        "–û–ø–∏—Ä–∞–π—Å—è –Ω–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ì–û–°–¢, –°–¢–û, –†–î. "
        "–§–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É, –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–æ–≤ –Ω–∞ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–∞—Ö."
    )

    wc = len(query.split())
    mode = (
        "reflective" if wc <= 3
        else "structured" if wc <= 15
        else "deep_analytic"
    )
    print(f"[üß©] Mode: {mode}")

    prompt = f"""{intro}

–í–æ–ø—Ä–æ—Å: {query}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–û—Ç–≤–µ—Ç:"""

    ans = await ollama_generate(prompt)
    print(f"\n[üí°] –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω:\n{ans}")
    log_event(f"[ANSWER | {mode}] {ans}\n{'='*80}\n")

    torch.cuda.empty_cache()
    return ans


def process_query(query: str, mode: str = "structured"):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –≤—ã–∑–æ–≤–∞ RAG –∏–∑ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª–µ–π (API, UI).
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        if not query or not query.strip():
            return "[‚ö†Ô∏è] –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å"

        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        result = loop.run_until_complete(answer(query))
        loop.close()

        return result
    except Exception as e:
        return f"[‚ùå] –û—à–∏–±–∫–∞ process_query: {e}"

# === CLI ===
if __name__ == "__main__":
    check_collection()
    q = "–∞—É–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Å–æ–±—ã—Ç–∏–π"
    asyncio.run(answer(q))
