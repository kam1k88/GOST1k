import sys; sys.stdout.reconfigure(encoding='utf-8')
import os
import re
import asyncio
import httpx
import numpy as np
import time
from datetime import datetime
from dotenv import load_dotenv
from FlagEmbedding import BGEM3FlagModel
from sentence_transformers import CrossEncoder
import chromadb
import torch

# === –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ===
os.environ["CHROMA_TELEMETRY"] = "False"
load_dotenv()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_DIR = os.path.join(BASE_DIR, "chroma_db")
COLLECTION_NAME = "gost1k"
LOG_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, "gost1k.log")

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
def log_event(msg):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{ts}] {msg}\n")

# === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ ===
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"[‚öôÔ∏è] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE.upper()}")

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π ===
embedder = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True, device=DEVICE)
reranker = CrossEncoder("BAAI/bge-reranker-base", device=DEVICE)

# === –ü—Ä–æ–≥—Ä–µ–≤ GPU ===
_ = embedder.encode(["warmup"], batch_size=1, max_length=128)
torch.cuda.empty_cache()

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Chroma ===
client = chromadb.PersistentClient(path=CHROMA_DIR)
collection = client.get_or_create_collection(COLLECTION_NAME, embedding_function=None)

# === –£—Ç–∏–ª–∏—Ç—ã ===
def clean_text(t):
    t = re.sub(r"<[^>]+>", " ", t)
    t = re.sub(r"\s+", " ", t)
    return t.strip()

def normalize_query(q: str) -> str:
    return f"query: {q.strip().lower()}"

def format_doc(d: str) -> str:
    return f"passage: {d.strip()}"

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ===
def check_collection():
    try:
        print("[üß©] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Chroma...")
        count = collection.count()
        print(f"[üì¶] –í—Å–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {count}")
        peek = collection.peek()
        if not peek or "documents" not in peek or not peek["documents"]:
            print("[‚ö†Ô∏è] –ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞.")
            return
        doc = peek["documents"][0][0] if isinstance(peek["documents"][0], list) else peek["documents"][0]
        meta = peek["metadatas"][0][0] if isinstance(peek["metadatas"][0], list) else peek["metadatas"][0]
        print("[üìÑ] –ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n", clean_text(doc[:300]))
        print("[üóÇÔ∏è] –ü—Ä–∏–º–µ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:\n", meta)
    except Exception as e:
        print(f"[‚ùå] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")

# === –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (dense + sparse fusion) ===
def hybrid_search(q, top_k=50, alpha=0.65):
    """
    alpha - –≤–µ—Å dense. 0.65 –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –ì–û–°–¢–æ–≤ (—Å–µ–º–∞–Ω—Ç–∏–∫–∞ –≤–∞–∂–Ω–µ–µ, –Ω–æ —Ç–µ—Ä–º–∏–Ω—ã —É—á–∏—Ç—ã–≤–∞–µ–º).
    """
    try:
        t0 = time.time()

        # 1) –≥–∏–±—Ä–∏–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–ø—Ä–æ—Å–∞
        #   –≤ FlagEmbedding>=1.3 –Ω–µ –Ω—É–∂–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å normalize_embeddings,
        #   –∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å return_dense, return_sparse
        res = embedder.encode([q], return_dense=True, return_sparse=True)
        dense_vec = res["dense_vecs"]
        sparse_weights = res["lexical_weights"][0] if "lexical_weights" in res else {}

        # 2) dense-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –∏–∑ Chroma
        cres = collection.query(query_embeddings=dense_vec, n_results=top_k * 2)
        docs0 = cres.get("documents", [[]])[0]
        metas0 = cres.get("metadatas", [[]])[0]
        dists0 = cres.get("distances", [[]])[0]

        if not docs0:
            print("[‚ö†Ô∏è] –ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.")
            return []

        # === dense part ===
        dense_scores = {}
        for m, dist in zip(metas0, dists0):
            sim = 1.0 - float(dist)
            dense_scores[m["source"]] = max(dense_scores.get(m["source"], 0.0), sim)

        # === sparse part ===
        sparse_scores = {}
        for doc, meta in zip(docs0, metas0):
            if not sparse_weights:
                continue
            score = sum(sparse_weights.get(tok, 0.0) for tok in doc.split())
            sparse_scores[meta["source"]] = max(sparse_scores.get(meta["source"], 0.0), score)

        # === fusion ===
        fused = {}
        keys = set(dense_scores.keys()) | set(sparse_scores.keys())
        for k in keys:
            fused[k] = alpha * dense_scores.get(k, 0.0) + (1 - alpha) * sparse_scores.get(k, 0.0)

        # === –æ—Ç–ª–∞–¥–æ—á–Ω—ã–µ —Ç–æ–ø—ã ===
        if fused:
            top_dense = sorted(dense_scores.items(), key=lambda x: x[1], reverse=True)[:3]
            top_sparse = sorted(sparse_scores.items(), key=lambda x: x[1], reverse=True)[:3]
            print("\nüß† Top dense:")
            [print(f"  {n}: {s:.3f}") for n, s in top_dense]
            print("ü™∂ Top sparse:")
            [print(f"  {n}: {s:.3f}") for n, s in top_sparse]

        # === —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥ ===
        ranked_ids = [k for k, _ in sorted(fused.items(), key=lambda x: x[1], reverse=True)[:top_k]]
        docs = [{"text": d, "source": s} for d, s in zip(docs0, metas0) if s["source"] in ranked_ids]

        total_time = time.time() - t0
        log_event(f"[‚è±Ô∏è] BGE-M3 hybrid dense+sparse: {total_time:.2f} —Å–µ–∫ ({len(docs)} docs)")
        return docs

    except Exception as e:
        print(f"[‚ùå] –û—à–∏–±–∫–∞ hybrid_search: {e}")
        log_event(f"[‚ùå] –û—à–∏–±–∫–∞ hybrid_search: {e}")
        return []

# === –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ ===
def rerank_docs(q, docs):
    if not docs:
        return []
    t0 = time.time()
    pairs = [[normalize_query(q), format_doc(d["text"])] for d in docs]
    scores = reranker.predict(pairs)
    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    rerank_time = time.time() - t0
    log_event(f"[‚è±Ô∏è] Rerank: {rerank_time:.2f} —Å–µ–∫ ({len(docs)} docs)")
    return [r[0] for r in ranked]

# === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Ollama ===
async def ollama_generate(prompt, model=None):
    torch.cuda.empty_cache()
    model = model or os.getenv("OLLAMA_MODEL", "qwen2.5:7b-instruct-q4_K_M")
    async with httpx.AsyncClient(timeout=300.0) as c:
        try:
            ollama_host = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
            t0 = time.time()
            r = await c.post(f"{ollama_host}/api/generate",
                             json={"model": model, "prompt": prompt, "stream": False})
            data = r.json()
            total_time = time.time() - t0
            log_event(f"[‚è±Ô∏è] Ollama –æ—Ç–≤–µ—Ç: {total_time:.2f} —Å–µ–∫")
            if "response" in data and data["response"].strip():
                return data["response"].strip()
            return data.get("output", "") or str(data)
        except Exception as e:
            return f"[‚ö†Ô∏è] Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}"
        finally:
            torch.cuda.empty_cache()

# === –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ RAG ===
async def answer(query: str):
    t0 = time.time()
    print(f"[üîç] –ó–∞–ø—Ä–æ—Å: {query}")
    sys_query = normalize_query(query)
    log_event(f"\n[USER QUERY] {query}")
    log_event(f"[SYSTEM QUERY] {sys_query}")

    # === 1. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ===
    docs = hybrid_search(sys_query)
    print(f"[~] –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(docs)}")

    if not docs:
        msg = "[‚ö†Ô∏è] –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."
        print(msg)
        log_event(msg)
        return msg

    # === 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è (intent detection) ===
    q_lower = query.lower()
    num_docs = len(docs)
    mode = "structured"  # –±–∞–∑–æ–≤—ã–π —Ä–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    if any(w in q_lower for w in ["–ø–µ—Ä–µ—á–∏—Å–ª–∏", "–≤—Å–µ –≥–æ—Å—Ç", "–≤—Å–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã", "—Å–ø–∏—Å–æ–∫", "–ø–µ—Ä–µ—á–µ–Ω—å"]) or num_docs > 15:
        mode = "list"
        top_k = min(num_docs, 30)
    elif any(w in q_lower for w in ["—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "–∫—Ä–∏—Ç–µ—Ä–∏–∏", "—É—Å–ª–æ–≤–∏—è", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"]):
        mode = "criteria"
        top_k = min(num_docs, 8)
    elif any(w in q_lower for w in ["–ø–∏—Å—å–º–æ", "–ø—Ä–∏–º–µ—Ä", "—Ç–µ–∫—Å—Ç", "–¥–æ–∫—É–º–µ–Ω—Ç", "—Å–æ—Å—Ç–∞–≤—å", "–Ω–∞–ø–∏—à–∏"]):
        mode = "long"
        top_k = min(num_docs, 10)
    elif any(w in q_lower for w in ["—Å—Ä–∞–≤–Ω–∏", "—Ä–∞–∑–ª–∏—á–∏—è", "–æ—Ç–ª–∏—á–∏–µ", "vs"]):
        mode = "compare"
        top_k = min(num_docs, 12)
    elif any(w in q_lower for w in ["–∞–Ω–∞–ª–∏–∑", "–æ–±—ä—è—Å–Ω–∏", "–ø–æ—è—Å–Ω–∏", "—Å–≤—è–∑—å", "–≤–ª–∏—è–Ω–∏–µ", "–∫–∞–∫ —Å–≤—è–∑–∞–Ω–æ"]) or len(q_lower.split()) > 20:
        mode = "analytic"
        top_k = min(num_docs, 15)
    elif any(w in q_lower for w in ["–∫—Ä–∞—Ç–∫–æ", "—Ä–µ–∑—é–º–µ", "–≤ –¥–≤—É—Ö —Å–ª–æ–≤–∞—Ö", "–∏—Ç–æ–≥"]):
        mode = "summary"
        top_k = min(num_docs, 5)
    else:
        mode = "structured"
        top_k = min(num_docs, 7)

    print(f"[‚öôÔ∏è] –†–µ–∂–∏–º: {mode}, top_docs={top_k}")

    # === 3. –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ ===
    reranked = rerank_docs(sys_query, docs)
    top_docs = reranked[:top_k]
    context = "\n\n".join([clean_text(d["text"][:800]) for d in top_docs])

    # === 4. –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è LLM ===
    intros = {
        "structured": "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –¢–ö 362 '–ó–∞—â–∏—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏'. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —Ç–æ—á–Ω–æ –∏ –ø–æ —Ñ–∞–∫—Ç–∞–º, —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ì–û–°–¢–æ–≤.",
        "criteria": "–ö—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—á–∏—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–ª–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ. –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏ —É–∫–∞–∂–∏ –ì–û–°–¢—ã, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å.",
        "list": "–ü–µ—Ä–µ—á–∏—Å–ª–∏ –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã (–ì–û–°–¢, –°–¢–û, ISO –∏ —Ç.–¥.) —Å –∏—Ö –Ω–æ–º–µ—Ä–∞–º–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏. –ù–µ –¥–∞–≤–∞–π –ø–æ—è—Å–Ω–µ–Ω–∏–π ‚Äî —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫.",
        "long": "–°–æ—Å—Ç–∞–≤—å —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –ø—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ø–∏—Å—å–º–∞ –∏–ª–∏ –ø–æ—è—Å–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º —Å—Ç–∏–ª–µ, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –¢–ö 362.",
        "compare": "–°—Ä–∞–≤–Ω–∏ —É–ø–æ–º—è–Ω—É—Ç—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∏–ª–∏ –ø–æ–¥—Ö–æ–¥—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º –∞—Å–ø–µ–∫—Ç–∞–º. –£–∫–∞–∂–∏ —Ä–∞–∑–ª–∏—á–∏—è –∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å–ø–∏—Å–∫–∞.",
        "analytic": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –∏ —Å–º—ã—Å–ª —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π, —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≤ –ì–û–°–¢–∞—Ö. –ò—Å–ø–æ–ª—å–∑—É–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –Ω–æ –æ—Å—Ç–∞–≤–∞–π—Å—è –≤ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Ä–∞–º–∫–∞—Ö. –ü—Ä–∏–≤–æ–¥–∏ –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å.",
        "summary": "–ö—Ä–∞—Ç–∫–æ –ø–æ–¥–≤–µ–¥–∏ –∏—Ç–æ–≥–∏ –ø–æ —Ç–µ–º–µ, –≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö, –±–µ–∑ –¥–µ—Ç–∞–ª–µ–π –∏ –Ω—É–º–µ—Ä–∞—Ü–∏–∏."
    }
    intro = intros.get(mode, intros["structured"])

    prompt = f"{intro}\n\n–í–æ–ø—Ä–æ—Å: {query}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–û—Ç–≤–µ—Ç:"

    # === 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Ollama ===
    ans = await ollama_generate(prompt)
    total_time = time.time() - t0
    print(f"\n[üí°] –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω ({total_time:.2f} —Å–µ–∫, —Ä–µ–∂–∏–º={mode}):\n{ans}")
    log_event(f"[ANSWER | {mode}] ({total_time:.2f} —Å–µ–∫)\n{ans}\n{'='*80}\n")
    return ans


# === CLI ===
if __name__ == "__main__":
    check_collection()
    q = "–∞—É–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Å–æ–±—ã—Ç–∏–π"
    asyncio.run(answer(q))
