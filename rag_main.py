import os
import chromadb
from sentence_transformers import SentenceTransformer
from openai import OpenAI

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
CHROMA_DIR = r"C:\Users\kam1k88\GOST1k\chroma_db"
EMBEDDING_MODEL = "intfloat/multilingual-e5-small"
LLM_MODEL = "Qwen2:7B-Instruct"  # –∏–º—è –º–æ–¥–µ–ª–∏ –≤ Ollama
TOP_K = 8  # —Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–¥–∞–≤–∞—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç

# –û—Ç–∫–ª—é—á–∞–µ–º —Ç–µ–ª–µ–º–µ—Ç—Ä–∏—é Chroma
os.environ["CHROMA_TELEMETRY_ENABLED"] = "false"

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π ===
embedding_model = SentenceTransformer(EMBEDDING_MODEL, device="cuda")
client_chroma = chromadb.PersistentClient(path=CHROMA_DIR)
collection = client_chroma.get_or_create_collection("gost1k")

# –ü–æ–¥–∫–ª—é—á–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π Ollama API (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π)
client_llm = OpenAI(base_url="http://localhost:11434/v1", api_key="ollama")

print(f"[‚úì] –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Chroma DB: {CHROMA_DIR}")
print(f"[‚úì] –ö–æ–ª–ª–µ–∫—Ü–∏—è: gost1k (—Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∞)")
print(f"[‚úì] –ú–æ–¥–µ–ª—å LLM: {LLM_MODEL}\n")

# === –§—É–Ω–∫—Ü–∏—è RAG-–ø–æ–∏—Å–∫–∞ ===
def rag_query(query: str, top_k: int = TOP_K):
    print(f"üîç –ó–∞–ø—Ä–æ—Å: {query}\n")

    # 1Ô∏è‚É£ –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    q_emb = embedding_model.encode([query], normalize_embeddings=True, convert_to_numpy=True)
    q_emb = q_emb.tolist() if hasattr(q_emb, "tolist") else q_emb

    # 2Ô∏è‚É£ –î–µ–ª–∞–µ–º –ø–æ–∏—Å–∫ –≤ Chroma
    results = collection.query(
        query_embeddings=q_emb,
        n_results=top_k,
        include=["documents", "metadatas"]
    )

    docs = results["documents"][0]
    metas = results["metadatas"][0]

    # 3Ô∏è‚É£ –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    context_blocks = []
    for i, (doc, meta) in enumerate(zip(docs, metas), 1):
        context_blocks.append(f"[{i}] –ò—Å—Ç–æ—á–Ω–∏–∫: {meta.get('source', '‚Äî')}\n{doc[:1000]}")
    context = "\n\n".join(context_blocks)

    # 4Ô∏è‚É£ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ LLM (—á–µ—Ä–µ–∑ Ollama)
    prompt = f"""
–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Ä–∞–±–æ—Ç–∞—é—â–∏–π —Å –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.
–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ì–û–°–¢–æ–≤ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —è–≤–Ω–æ —É–∫–∞–∂–∏ —ç—Ç–æ.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {query}
–û—Ç–≤–µ—Ç:
"""
    response = client_llm.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=800
    )

    answer = response.choices[0].message.content.strip()
    print("\nüß† –û—Ç–≤–µ—Ç LLM:\n")
    print(answer)

    return answer


# === –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ ===
if __name__ == "__main__":
    rag_query("—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∑–∞—â–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
