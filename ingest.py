import os
import fitz  # PyMuPDF
from docx import Document
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import chromadb

# === ÐŸÑƒÑ‚Ð¸ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ ===
CHROMA_DIR = "./chroma_db"
DOCS_DIR = "./docs"
COLLECTION_NAME = "gost1k"
BATCH_SIZE = 3000  # Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Chroma

# === Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° ===
client = chromadb.PersistentClient(path=CHROMA_DIR)
embedder = SentenceTransformer("intfloat/multilingual-e5-small", device="cuda")

# === Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐºÑÑ‚Ð° ===
def extract_text_from_pdf(path):
    text = ""
    try:
        with fitz.open(path) as doc:
            for page in doc:
                text += page.get_text("text")
    except Exception as e:
        print(f"[âš ï¸] ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ PDF {path}: {e}")
    return text

def extract_text_from_docx(path):
    try:
        doc = Document(path)
        return "\n".join([p.text for p in doc.paragraphs])
    except Exception as e:
        print(f"[âš ï¸] ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ DOCX {path}: {e}")
        return ""

# === Ð Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ðµ Ð½Ð° Ñ‡Ð°Ð½ÐºÐ¸ ===
def chunk_text(text, max_len=512):
    paras = [p.strip() for p in text.split("\n") if len(p.strip()) > 50]
    chunks, cur = [], ""
    for para in paras:
        if len(cur) + len(para) < max_len:
            cur += " " + para
        else:
            chunks.append(cur.strip())
            cur = para
    if cur:
        chunks.append(cur.strip())
    return chunks

# === Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ ===
def ingest_docs():
    collection = client.get_or_create_collection(COLLECTION_NAME)

    files = [f for f in os.listdir(DOCS_DIR) if f.lower().endswith((".pdf", ".docx"))]
    print(f"[â„¹ï¸] ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(files)} Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð¢Ðš 362 Ð´Ð»Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸.")

    all_chunks, all_meta = [], []

    for file in tqdm(files):
        path = os.path.join(DOCS_DIR, file)
        text = ""
        if file.lower().endswith(".pdf"):
            text = extract_text_from_pdf(path)
        elif file.lower().endswith(".docx"):
            text = extract_text_from_docx(path)

        if not text.strip():
            print(f"[âš ï¸] ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½ Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ñ„Ð°Ð¹Ð»: {file}")
            continue

        chunks = chunk_text(text)
        for i, chunk in enumerate(chunks):
            all_chunks.append("passage: " + chunk)
            all_meta.append({
                "source": file,
                "chunk_id": i,
                "abs_path": path
            })

    print(f"[+] Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾ {len(all_chunks)} Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð². Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸...")

    # === Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð² Ð¸ upsert Ð¿Ð¾ Ð±Ð°Ñ‚Ñ‡Ð°Ð¼ ===
    total = len(all_chunks)
    for i in range(0, total, BATCH_SIZE):
        batch_docs = all_chunks[i:i+BATCH_SIZE]
        batch_meta = all_meta[i:i+BATCH_SIZE]
        batch_ids = [f"id_{i+j}" for j in range(len(batch_docs))]

        embeddings = embedder.encode(batch_docs, normalize_embeddings=True)
        collection.upsert(
            ids=batch_ids,
            documents=batch_docs,
            metadatas=batch_meta,
            embeddings=embeddings
        )
        print(f"[ðŸ“¦] Ð˜Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ {i+len(batch_docs)}/{total} Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð²")

    print(f"[âœ“] Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°. Ð’ÑÐµÐ³Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(files)}, Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð²: {len(all_chunks)}")

# === ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ ===
def rebuild():
    print("[â™»ï¸] Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€ÑƒÑŽ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ...")
    try:
        client.delete_collection(COLLECTION_NAME)
    except Exception as e:
        print(f"[âš ï¸] ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ð¸: {e}")

    print("[ðŸ†•] Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð½Ð¾Ð²ÑƒÑŽ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ...")
    _ = client.get_or_create_collection(COLLECTION_NAME)
    ingest_docs()

# === CLI ===
if __name__ == "__main__":
    import sys
    if "--rebuild" in sys.argv:
        rebuild()
    else:
        ingest_docs()
