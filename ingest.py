import os
import re
import time
import torch
import chromadb
import shutil
import gc

from tqdm import tqdm
from datetime import datetime
from FlagEmbedding import BGEM3FlagModel
from docx import Document
from PyPDF2 import PdfReader

# === ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DOCS_DIR = os.path.join(BASE_DIR, "docs")
CHROMA_DIR = os.path.join(BASE_DIR, "chroma_db")
COLLECTION_NAME = "gost1k"
os.makedirs(CHROMA_DIR, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_NAME = "BAAI/bge-m3"
BATCH_SIZE = 32
CHUNK_SIZE = 512
CHUNK_OVERLAP = 128
MIN_LEN = 64
MAX_LEN = 2048

# === Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ===
def log(msg, icon="ðŸ’¬"):
    ts = datetime.now().strftime("%H:%M:%S")
    print(f"[{ts}] [{icon}] {msg}")

# === ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† Ð¸ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ñ Ñ‡Ð°Ð½ÐºÐ¾Ð² ===
def get_pdf_page_count(path):
    try:
        reader = PdfReader(path)
        return len(reader.pages)
    except Exception:
        return 1

def adapt_chunk_params(pdf_path):
    pages = get_pdf_page_count(pdf_path)
    if pages > 250:
        chunk_size, overlap, label = 1024, 256, "ðŸ“˜ Ð‘Ð¾Ð»ÑŒÑˆÐ¾Ð¹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚"
    elif pages > 80:
        chunk_size, overlap, label = 768, 192, "ðŸ“— Ð¡Ñ€ÐµÐ´Ð½Ð¸Ð¹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚"
    else:
        chunk_size, overlap, label = 512, 128, "ðŸ“™ ÐœÐ°Ð»Ñ‹Ð¹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚"
    log(f"{label}: {os.path.basename(pdf_path)} â†’ CHUNK={chunk_size}, OVERLAP={overlap}")
    return chunk_size, overlap

# === ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð° ===
def clean_text(text: str) -> str:
    text = re.sub(r"ÐšÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ð½Ñ‚ÐŸÐ»ÑŽÑ.*?(?=\n|$)", "", text, flags=re.IGNORECASE)
    text = re.sub(r"www\.consultant\.ru", "", text)
    text = re.sub(r"ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°\s*\d+\s*(Ð¸Ð·)?\s*\d*", "", text, flags=re.IGNORECASE)
    text = re.sub(r"Ð½Ð°Ð´ÐµÐ¶Ð½Ð°Ñ Ð¿Ñ€Ð°Ð²Ð¾Ð²Ð°Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ°", "", text, flags=re.IGNORECASE)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# === Ð Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐºÑÑ‚Ð° ===
def smart_split(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    text = clean_text(text)
    pattern = r"(?=(?:^|\s)(?:\d+\.\d+\.?\d*|[A-ZÐ-Ð¯]\.[\d\.]+|[a-zÐ°-Ñ]\)|[a-f]\)|[A-F]\))\s)"
    chunks = re.split(pattern, text)
    chunks = [c.strip() for c in chunks if len(c.strip()) > MIN_LEN]
    result, buf = [], ""
    for chunk in chunks:
        if len(buf) + len(chunk) < chunk_size:
            buf += " " + chunk
        else:
            if len(buf) > MIN_LEN:
                result.append(buf.strip())
            buf = chunk
    if buf:
        result.append(buf.strip())
    return [c for c in result if len(c) <= MAX_LEN]

# === Ð§Ñ‚ÐµÐ½Ð¸Ðµ Ñ„Ð°Ð¹Ð»Ð¾Ð² ===
def read_file(path):
    text = ""
    ext = os.path.splitext(path)[1].lower()
    try:
        if ext in [".txt", ".md"]:
            with open(path, "r", encoding="utf-8") as f:
                text = f.read()
        elif ext == ".docx":
            doc = Document(path)
            text = "\n".join(p.text for p in doc.paragraphs)
        elif ext == ".pdf":
            pdf = PdfReader(path)
            text = "\n".join(page.extract_text() or "" for page in pdf.pages)
    except Exception as e:
        log(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ {os.path.basename(path)}: {e}", "âš ï¸")
    return clean_text(text)

# === Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÐ² Ð¼Ð¾Ð´ÐµÐ»Ð¸ ===
# === Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ ===
def ingest_docs(rebuild=True):
    log(f"{'ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¿ÐµÑ€ÐµÐ¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ' if rebuild else 'Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²'} GOST1k...", "ðŸ§±")
    log(f"Ð£ÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾: {DEVICE}", "âš™ï¸")

    # === Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÐ² Ð¼Ð¾Ð´ÐµÐ»Ð¸ ===
    gpu_name = "CPU"
    vram_gb = 0.0
    if torch.cuda.is_available():
        props = torch.cuda.get_device_properties(0)
        gpu_name = torch.cuda.get_device_name(0)
        vram_gb = round(props.total_memory / 1024**3, 1)
    log(f"ðŸ§  GPU: {gpu_name} ({vram_gb} Ð“Ð‘ VRAM) | FP16: {'Yes' if torch.cuda.is_available() else 'No'} | BGE-M3 hybrid", "âš™ï¸")

    t0 = time.time()
    embedder = BGEM3FlagModel(MODEL_NAME, use_fp16=True, device=DEVICE)
    try:
        _ = embedder.encode(["warmup"], batch_size=1, max_length=128)
        warm = time.time() - t0
        log(f"ðŸ”¥ ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑ‚Ð° Ð·Ð° {warm:.1f} Ñ. Ð’ÐµÑÐ° Ð·Ð°ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð² HuggingFace.", "âœ…")
    except Exception as e:
        log(f"âš ï¸ ÐŸÑ€Ð¾Ð³Ñ€ÐµÐ² Ð½Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½: {e}", "âš ï¸")

    client = chromadb.PersistentClient(path=CHROMA_DIR)

    if rebuild:
        try:
            client.delete_collection(COLLECTION_NAME)
            log("Ð¡Ñ‚Ð°Ñ€Ð°Ñ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð° (Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸)", "ðŸ—‘ï¸")
            for d in os.listdir(CHROMA_DIR):
                path = os.path.join(CHROMA_DIR, d)
                if os.path.isdir(path) and re.match(r"^[0-9a-f\-]{36}$", d):
                    shutil.rmtree(path, ignore_errors=True)
                    log(f"Ð£Ð´Ð°Ð»ÐµÐ½Ð° Ð¿Ð°Ð¿ÐºÐ° {d}", "ðŸ§¹")
        except Exception as e:
            log(f"ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¾Ð²Ð°Ð»Ð° â€” ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð·Ð°Ð½Ð¾Ð²Ð¾ ({e})", "âš ï¸")

    collection = client.get_or_create_collection(name=COLLECTION_NAME)
    files = [f for f in os.listdir(DOCS_DIR) if f.lower().endswith((".txt", ".md", ".docx", ".pdf"))]
    log(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð²: {len(files)}", "ðŸ“„")

    total_chunks = 0
    start_time = time.time()

    for file in tqdm(files, desc="ðŸ” Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ", ncols=100):
        path = os.path.join(DOCS_DIR, file)
        if path.lower().endswith(".pdf"):
            chunk_size, overlap = adapt_chunk_params(path)
        else:
            chunk_size, overlap = CHUNK_SIZE, CHUNK_OVERLAP

        text = read_file(path)
        if not text or len(text) < MIN_LEN:
            continue

        chunks = smart_split(text, chunk_size, overlap)
        if not chunks:
            continue

        embeddings = []
        cur_batch = BATCH_SIZE

        for i in range(0, len(chunks), cur_batch):
            batch = chunks[i:i + cur_batch]
            while True:
                try:
                    res = embedder.encode(batch, batch_size=cur_batch, max_length=2048)
                    dense_vecs = res["dense_vecs"]
                    embeddings.extend(dense_vecs)
                    break
                except RuntimeError as e:
                    if "CUDA out of memory" in str(e):
                        torch.cuda.empty_cache()
                        gc.collect()
                        cur_batch = max(1, cur_batch // 2)
                        log(f"âš ï¸ OOM â€” ÑƒÐ¼ÐµÐ½ÑŒÑˆÐ°ÑŽ batch Ð´Ð¾ {cur_batch}", "âš ï¸")
                        time.sleep(1)
                    else:
                        raise

        ids = [f"{file}_{i}" for i in range(len(chunks))]
        metas = [{"source": file, "chunk": i} for i in range(len(chunks))]
        collection.add(documents=chunks, embeddings=embeddings, ids=ids, metadatas=metas)

        total_chunks += len(chunks)
        vram_now = torch.cuda.memory_allocated() / 1024**2
        log(f"[âœ…] {file}: {len(chunks)} Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð² | VRAM: {vram_now:.0f} ÐœÐ‘", "ðŸ“¦")

        torch.cuda.empty_cache()
        gc.collect()

    elapsed = time.time() - start_time
    log(f"Ð’ÑÐµÐ³Ð¾ Ð¿Ñ€Ð¾Ð¸Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¾: {total_chunks} Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ð·Ð° {elapsed:.1f} ÑÐµÐº", "ðŸ“¦")
    log("Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°", "âœ…")

if __name__ == "__main__":
    ingest_docs(rebuild=True)
    input("Ð”Ð»Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð° Ð½Ð°Ð¶Ð¼Ð¸Ñ‚Ðµ Enter...")
